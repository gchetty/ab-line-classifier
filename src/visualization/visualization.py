import datetime
import os
import io
import yaml
import math
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import tensorflow as tf
from sklearn.metrics import confusion_matrix, roc_curve, auc
from skopt.plots import plot_objective
from pandas.api.types import is_numeric_dtype

mpl.rcParams['figure.figsize'] = (12, 8)
cfg = yaml.full_load(open(os.getcwd() + "/config.yml", 'r'))

def plot_to_tensor():
    '''
    Converts a matplotlib figure to an image tensor
    :param figure: A matplotlib figure
    :return: Tensorflow tensor representing the matplotlib image
    '''
    # Save the plot to a PNG in memory.
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)

    image_tensor = tf.image.decode_png(buf.getvalue(), channels=4)     # Convert .png buffer to tensorflow image
    image_tensor = tf.expand_dims(image_tensor, 0)     # Add the batch dimension
    return image_tensor

def visualize_heatmap(orig_img, heatmap, img_filename, label, probs, class_names, dir_path=None):
    '''
    Obtain a comparison of an original image and heatmap produced by Grad-CAM.
    :param orig_img: Original X-Ray image
    :param heatmap: Heatmap generated by Grad-CAM.
    :param img_filename: Filename of the image explained
    :param label: Ground truth class of the example
    :param probs: Prediction probabilities
    :param class_names: Ordered list of class names
    :param dir_path: Path to save the generated image
    :return: Path to saved image
    '''

    fig, ax = plt.subplots(1, 2)
    ax[0].imshow(orig_img)
    ax[1].imshow(heatmap)

    # Display some information about the example
    pred_class = np.argmax(probs)
    fig.text(0.02, 0.90, "Prediction probabilities for: " + str(class_names) + ': ' +
             str(['{:.2f}'.format(probs[i]) for i in range(len(probs))]), fontsize=10)
    fig.text(0.02, 0.92, "Predicted Class: " + str(pred_class) + ' (' + class_names[pred_class] + ')', fontsize=10)
    if label is not None:
        fig.text(0.02, 0.94, "Ground Truth Class: " + str(label) + ' (' + class_names[label] + ')', fontsize=10)
    fig.suptitle("Grad-CAM heatmap for image " + img_filename, fontsize=8, fontweight='bold')
    fig.tight_layout()

    # Save the image
    filename = None
    if dir_path is not None:
        filename = os.path.join(dir_path, img_filename.split('/')[-1] + '_gradcam_' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')
        plt.savefig(filename)
    return filename


def plot_roc(labels, predictions, class_name_list, dir_path=None, title=None):
    '''
    Plots the ROC curve for predictions on a dataset
    :param labels: Ground truth labels
    :param predictions: Model prediction probabilities corresponding to the labels
    :param class_name_list: Ordered list of class names
    :param dir_path: Directory in which to save image
    '''
    plt.clf()
    for class_id in range(len(class_name_list)):
        class_name = class_name_list[class_id]
        single_class_preds = predictions[:, class_id]    # Only care about one class
        single_class_labels = (np.array(labels) == class_id) * 1.0
        fp, tp, _ = roc_curve(single_class_labels, single_class_preds)  # Get values for true positive and true negative
        plt.plot(100*fp, 100*tp, label=class_name, linewidth=2)   # Plot the ROC curve

    if title is None:
        plt.title('ROC curves for test set')
    else:
        plt.title(title)
    plt.xlabel('False positives [%]')
    plt.ylabel('True positives [%]')
    plt.xlim([-5,105])
    plt.ylim([-5,105])
    plt.grid(True)
    plt.legend()
    ax = plt.gca()
    ax.set_aspect('equal')
    if dir_path is not None:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        plt.savefig(dir_path + 'ROC_' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')
    return plt


def plot_confusion_matrix(labels, predictions, class_name_list, dir_path=None, title=None):
    '''
    Plot a confusion matrix for the ground truth labels and corresponding model predictions for a particular class.
    :param labels: Ground truth labels
    :param predictions: Model predictions
    :param class_name_list: Ordered list of class names
    :param dir_path: Directory in which to save image
    '''
    plt.clf()
    predictions = list(np.argmax(predictions, axis=1))
    ax = plt.subplot()
    cm = confusion_matrix(list(labels), predictions)  # Determine confusion matrix
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)  # Plot confusion matrix
    ax.figure.colorbar(im, ax=ax)
    ax.set(yticklabels=class_name_list, xticklabels=class_name_list)
    ax.xaxis.set_major_locator(mpl.ticker.IndexLocator(base=1, offset=0.5))
    ax.yaxis.set_major_locator(mpl.ticker.IndexLocator(base=1, offset=0.5))

    # Print the confusion matrix numbers in the center of each cell of the plot
    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    # Set plot's title and axis names
    if title is None:
        plt.title('Confusion matrix for test set')
    else:
        plt.title(title)
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')

    # Save the image
    if dir_path is not None:
        plt.savefig(dir_path + 'CM_' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')

    print('Confusion matrix: ', cm)    # Print the confusion matrix
    return plt


def plot_bayesian_hparam_opt(model_name, hparam_names, search_results, save_fig=False):
    '''
    Plot all 2D hyperparameter comparisons from the logs of a Bayesian hyperparameter optimization.
    :param model_name: Name of the model
    :param hparam_names: List of hyperparameter identifiers
    :param search_results: The object resulting from a Bayesian hyperparameter optimization with the skopt package
    :param save_fig:
    :return:
    '''

    # Abbreviate hyperparameters to improve plot readability
    axis_labels = hparam_names.copy()
    for i in range(len(axis_labels)):
        if len(axis_labels[i]) >= 12:
            axis_labels[i] = axis_labels[i][:4] + '...' + axis_labels[i][-4:]

    # Plot
    axes = plot_objective(result=search_results, dimensions=axis_labels)

    # Create a title
    fig = plt.gcf()
    fig.suptitle('Bayesian Hyperparameter\n Optimization for ' + model_name, fontsize=15, x=0.65, y=0.97)

    # Indicate which hyperparameter abbreviations correspond with which hyperparameter
    hparam_abbrs_text = ''
    for i in range(len(hparam_names)):
        hparam_abbrs_text += axis_labels[i] + ':\n'
    fig.text(0.50, 0.8, hparam_abbrs_text, fontsize=10, style='italic', color='mediumblue')
    hparam_names_text = ''
    for i in range(len(hparam_names)):
        hparam_names_text += hparam_names[i] + '\n'
    fig.text(0.65, 0.8, hparam_names_text, fontsize=10, color='darkblue')

    fig.tight_layout()
    if save_fig:
        plt.savefig(cfg['PATHS']['EXPERIMENT_VISUALIZATIONS'] + 'Bayesian_opt_' + model_name + '_' +
                    datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')


def plot_b_line_threshold_experiment(metrics_df, min_threshold, max_threshold, thresh_col, class_thresh, metrics_to_plot=None):
    '''
    Visualizes the Plot classification metrics for clip predictions over various B-line count thresholds.
    :param metrics_df: DataFrame containing classification metrics for different . The first column should be the
                       various B-line thresholds and the rest are classification metrics
    :param min_threshold: Minimum B-line threshold
    :param max_threshold: Maximum B-line threshold
    :thresh_col: Column of DataFrame corresponding to threshold variable
    :class_thresh: Classification threshold
    :param metrics_to_plot: List of metrics to include on the plot
    '''

    ax = plt.subplot()
    plt.title('Classification Metrics for Clip Predictions vs. ' + thresh_col + ' (classification threshold = ' + str(class_thresh) + ')')
    ax.set_xlabel(thresh_col)
    ax.set_ylim(0., 1.)

    if metrics_to_plot is None:
        metric_names = [m for m in metrics_df.columns if m != thresh_col and is_numeric_dtype(metrics_df[m])]
    else:
        metric_names = metrics_to_plot

    # Plot each metric as a separate series and place a legend
    for metric_name in metric_names:
        if is_numeric_dtype(metrics_df[metric_name]):
            ax.plot(metrics_df[thresh_col], metrics_df[metric_name])

    # Change axis ticks and add grid
    ax.minorticks_on()
    ax.set_xlim(min_threshold - 1, max_threshold + 1)
    ax.xaxis.set_ticks(np.arange(0, max_threshold + 1, 5))
    ax.yaxis.set_ticks(np.arange(0., 1.01, 0.05))
    ax.grid(True, which='both', color='lightgrey')

    # Draw legend
    ax.legend(metric_names, loc='lower right')

    plt.savefig(cfg['PATHS']['EXPERIMENT_VISUALIZATIONS'] + thresh_col +
                datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')


def plot_b_line_threshold_roc_curve(tprs, fprs):
    '''
    Plot ROC curve and determine AUC, given a list of true positive and false positive rates at a variety of thresholds.
    :param tprs: List of true positive rates
    :param fprs: List of false positive rates
    '''
    print(fprs)
    print(tprs)
    plt.clf()
    ax = plt.subplot()
    ax.plot(fprs, tprs, linewidth=3)  # Plot the ROC curve

    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    ax.set_xlim(0., 1.)
    ax.set_ylim(0., 1.)
    ax.xaxis.set_ticks(np.arange(0., 1.01, 0.1))
    ax.yaxis.set_ticks(np.arange(0., 1.01, 0.1))
    ax.grid(True, color='lightgrey')
    ax.set_aspect('equal')

    AUC = auc(fprs, tprs)
    plt.title("ROC for Varying B-line Thresholds (AUC={:.5f})".format(AUC))

    plt.savefig(cfg['PATHS']['EXPERIMENT_VISUALIZATIONS'] + 'roc_ct_' +
                datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + '.png')
